"""
å®Œæ•´çš„æ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰€æœ‰ç»„ä»¶çš„åŠŸèƒ½
"""

import numpy as np
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# å¯¼å…¥æ‰€æœ‰æ¨¡å—
from gpt_without_libs.models.layers.embedding import test_embedding
from gpt_without_libs.models.layers.positional_encoding import test_positional_encoding
from gpt_without_libs.models.layers.attention import test_multihead_attention
from gpt_without_libs.models.layers.feedforward import test_feedforward
from gpt_without_libs.models.layers.layer_norm import test_layer_norm
from gpt_without_libs.models.layers.transformer_block import test_transformer_block
from gpt_without_libs.models.core.mini_gpt import MiniGPT, test_mini_gpt


def test_all_components():
    """æµ‹è¯•æ‰€æœ‰ç»„ä»¶"""
    print("=" * 50)
    print("å¼€å§‹æµ‹è¯•æ‰€æœ‰ç»„ä»¶...")
    print("=" * 50)

    try:
        # æµ‹è¯•åŸºç¡€ç»„ä»¶
        test_embedding()
        test_positional_encoding()
        test_layer_norm()
        test_multihead_attention()
        test_feedforward()
        test_transformer_block()
        test_mini_gpt()

        print("=" * 50)
        print("æ‰€æœ‰ç»„ä»¶æµ‹è¯•é€šè¿‡ï¼âœ…")
        print("=" * 50)

        return True

    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_model_forward_pass():
    """æµ‹è¯•å®Œæ•´æ¨¡å‹çš„å‰å‘ä¼ æ’­"""
    print("\næµ‹è¯•å®Œæ•´æ¨¡å‹å‰å‘ä¼ æ’­...")

    from gpt_without_libs.models.core.mini_gpt import MiniGPT

    # åˆ›å»ºä¸€ä¸ªå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
    vocab_size = 100
    embed_dim = 32
    num_heads = 4
    num_layers = 2
    hidden_dim = 64

    model = MiniGPT(vocab_size, embed_dim, num_heads, num_layers, hidden_dim)

    # æµ‹è¯•è¾“å…¥
    batch_size = 1
    seq_len = 20
    x = np.random.randint(0, vocab_size, (batch_size, seq_len))

    # ç”Ÿæˆæ©ç 
    mask = model.generate_mask(seq_len)

    # å‰å‘ä¼ æ’­
    logits = model.forward(x, mask)

    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {logits.shape}")
    print(f"é¢„æœŸè¾“å‡ºå½¢çŠ¶: ({batch_size}, {seq_len}, {vocab_size})")

    assert logits.shape == (batch_size, seq_len, vocab_size), "è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…"

    # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åˆç†ï¼ˆä¸æ˜¯NaNæˆ–Infï¼‰
    assert not np.any(np.isnan(logits)), "è¾“å‡ºåŒ…å«NaN"
    assert not np.any(np.isinf(logits)), "è¾“å‡ºåŒ…å«Inf"

    print("å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼âœ…")
    return True


def test_model_backward_pass():
    """æµ‹è¯•å®Œæ•´æ¨¡å‹çš„åå‘ä¼ æ’­"""
    print("\næµ‹è¯•å®Œæ•´æ¨¡å‹åå‘ä¼ æ’­...")

    from gpt_without_libs.models.core.mini_gpt import MiniGPT

    # åˆ›å»ºæ¨¡å‹
    vocab_size = 100
    embed_dim = 32
    num_heads = 4
    num_layers = 2
    hidden_dim = 64

    model = MiniGPT(vocab_size, embed_dim, num_heads, num_layers, hidden_dim)

    # æµ‹è¯•æ•°æ®
    batch_size = 1
    seq_len = 10
    x = np.random.randint(0, vocab_size, (batch_size, seq_len))

    # å‰å‘ä¼ æ’­
    logits = model.forward(x)

    # æ¨¡æ‹ŸæŸå¤±æ¢¯åº¦ï¼ˆç®€å•çš„æ¢¯åº¦ï¼‰
    grad_logits = np.random.randn(*logits.shape) * 0.1

    # åå‘ä¼ æ’­
    grad_x = model.backward(grad_logits)

    print(f"è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {grad_x.shape}")
    print(f"é¢„æœŸè¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.shape}")

    assert grad_x.shape == x.shape, "è¾“å…¥æ¢¯åº¦å½¢çŠ¶ä¸åŒ¹é…"

    print("åå‘ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼âœ…")
    return True


def test_model_training_step():
    """æµ‹è¯•æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæ­¥éª¤"""
    print("\næµ‹è¯•æ¨¡å‹è®­ç»ƒæ­¥éª¤...")

    from gpt_without_libs.models.core.mini_gpt import MiniGPT

    # åˆ›å»ºæ¨¡å‹
    vocab_size = 100
    embed_dim = 32
    num_heads = 4
    num_layers = 2
    hidden_dim = 64

    model = MiniGPT(vocab_size, embed_dim, num_heads, num_layers, hidden_dim)

    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    batch_size = 2
    seq_len = 8
    x = np.random.randint(0, vocab_size, (batch_size, seq_len))
    y = np.random.randint(0, vocab_size, (batch_size, seq_len))

    learning_rate = 0.001

    # ä¿å­˜åˆå§‹å‚æ•°
    initial_weights = model.output_projection.copy()

    # å‰å‘ä¼ æ’­
    logits = model.forward(x)

    # è®¡ç®—ç®€å•çš„äº¤å‰ç†µæŸå¤±æ¢¯åº¦
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…è®­ç»ƒä¸­éœ€è¦æ›´å¤æ‚çš„æŸå¤±è®¡ç®—
    loss_grad = np.zeros_like(logits)
    for b in range(batch_size):
        for t in range(seq_len):
            loss_grad[b, t, y[b, t]] = -1.0 / vocab_size

    # åå‘ä¼ æ’­
    model.backward(loss_grad)

    # æ›´æ–°å‚æ•°
    model.update(learning_rate)

    # æ£€æŸ¥å‚æ•°æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–
    weight_change = np.abs(model.output_projection - initial_weights).mean()
    print(f"æƒé‡å¹³å‡å˜åŒ–: {weight_change:.6f}")

    assert weight_change > 1e-8, "æƒé‡æ²¡æœ‰æ›´æ–°"

    print("è®­ç»ƒæ­¥éª¤æµ‹è¯•é€šè¿‡ï¼âœ…")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹è¿è¡Œå®Œæ•´çš„æ¨¡å‹æµ‹è¯•...")

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    success = True

    try:
        success &= test_all_components()
        success &= test_model_forward_pass()
        success &= test_model_backward_pass()
        success &= test_model_training_step()

        if success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹åŠŸèƒ½æ­£å¸¸ã€‚")
            print("\næ¨¡å‹å‚æ•°ç»Ÿè®¡:")
            from gpt_without_libs.models.core.mini_gpt import MiniGPT
            model = MiniGPT(1000, 256, 8, 6, 1024)
            num_params = model.count_parameters()
            print(f"å®Œæ•´æ¨¡å‹å‚æ•°æ•°é‡: {num_params:,}")
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ã€‚")
            sys.exit(1)

    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()