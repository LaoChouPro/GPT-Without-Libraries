"""
æ”¹è¿›çš„è‹±æ–‡è¯­æ–™æ•°æ®é›†
åŒ…å«æ›´çœŸå®çš„è‹±æ–‡æ–‡æœ¬å’Œæ›´å¥½çš„åˆ†è¯ç­–ç•¥
"""

import numpy as np
import re
from typing import List, Dict, Tuple, Optional
import random


class EnglishTextDataset:
    """è‹±æ–‡æ–‡æœ¬æ•°æ®é›†ç”Ÿæˆå™¨"""

    def __init__(self):
        # å¸¸ç”¨è‹±æ–‡å•è¯åº“
        self.articles = ['the', 'a', 'an']
        self.pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they']
        self.nouns = [
            'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world',
            'life', 'hand', 'part', 'child', 'eye', 'woman', 'place', 'work',
            'week', 'case', 'point', 'government', 'company', 'number', 'group',
            'problem', 'fact', 'home', 'water', 'room', 'mother', 'area', 'money',
            'story', 'book', 'school', 'student', 'teacher', 'computer', 'house',
            'car', 'city', 'country', 'university', 'family', 'friend', 'team'
        ]
        self.verbs = [
            'be', 'have', 'do', 'say', 'go', 'get', 'make', 'know', 'think', 'take',
            'see', 'come', 'want', 'look', 'use', 'find', 'give', 'tell', 'work',
            'call', 'try', 'ask', 'need', 'feel', 'become', 'leave', 'put', 'mean',
            'keep', 'let', 'begin', 'seem', 'help', 'talk', 'turn', 'start', 'show',
            'hear', 'play', 'run', 'move', 'live', 'believe', 'hold', 'bring', 'happen',
            'write', 'provide', 'sit', 'stand', 'lose', 'pay', 'meet', 'include', 'continue'
        ]
        self.adjectives = [
            'good', 'new', 'first', 'last', 'long', 'great', 'little', 'own', 'other',
            'old', 'right', 'big', 'high', 'different', 'small', 'large', 'next', 'early',
            'young', 'important', 'few', 'public', 'bad', 'same', 'able', 'happy', 'sad',
            'beautiful', 'ugly', 'fast', 'slow', 'hot', 'cold', 'easy', 'difficult',
            'interesting', 'boring', 'exciting', 'relaxing', 'clean', 'dirty', 'expensive',
            'cheap', 'modern', 'traditional', 'popular', 'famous', 'unknown'
        ]
        self.prepositions = [
            'of', 'in', 'to', 'for', 'with', 'on', 'at', 'from', 'by', 'about',
            'as', 'into', 'like', 'through', 'after', 'over', 'between', 'out',
            'against', 'during', 'without', 'before', 'under', 'around', 'near'
        ]
        self.conjunctions = ['and', 'but', 'or', 'because', 'so', 'yet', 'for', 'nor']
        self.adverbs = [
            'up', 'so', 'out', 'just', 'now', 'how', 'then', 'more', 'also', 'here',
            'well', 'only', 'very', 'even', 'back', 'there', 'down', 'still', 'in',
            'as', 'too', 'when', 'never', 'really', 'most', 'again', 'always',
            'often', 'sometimes', 'usually', 'quickly', 'slowly', 'carefully',
            'happily', 'sadly', 'easily', 'difficultly', 'beautifully'
        ]

        # å¥å­æ¨¡æ¿
        self.sentence_patterns = [
            # ä¸»è°“å®¾
            ['article', 'noun', 'verb', 'article', 'noun'],
            # ä¸»ç³»è¡¨
            ['article', 'noun', 'verb', 'adjective'],
            # ä¸»è°“ä»‹å®¾
            ['article', 'noun', 'verb', 'preposition', 'article', 'noun'],
            # ä¸»è°“çŠ¶
            ['article', 'noun', 'verb', 'adverb'],
            # å¤åˆå¥
            ['pronoun', 'verb', 'article', 'noun', 'conjunction', 'pronoun', 'verb', 'adjective'],
            # ç–‘é—®å¥
            ['auxiliary', 'pronoun', 'verb', 'article', 'noun', 'question'],
            # å¦å®šå¥
            ['pronoun', 'auxiliary', 'not', 'verb', 'article', 'noun'],
        ]

        # é¢„å®šä¹‰çš„å¸¸ç”¨çŸ­è¯­å’Œå¥å­
        self.common_phrases = [
            "the weather is nice today",
            "i like to read books",
            "she works at the university",
            "they are playing in the garden",
            "he is a good student",
            "we have a beautiful house",
            "the children are happy",
            "my family lives in the city",
            "i enjoy studying computer science",
            "the company is growing fast",
            "she speaks three languages",
            "they travel around the world",
            "he plays football every weekend",
            "we need to solve this problem",
            "the teacher explains the lesson",
            "students work hard for exams",
            "technology changes our lives",
            "music makes me feel happy",
            "the sun rises in the east",
            "birds sing in the morning",
            "i drink coffee every morning",
            "she writes interesting stories",
            "they build modern houses",
            "the car needs more fuel",
            "we celebrate birthdays together",
            "nature is very beautiful",
            "computers help people work",
            "friends support each other",
            "learning new skills is important",
            "the internet connects everyone",
            "exercise keeps you healthy",
            "books contain valuable knowledge",
            "time passes quickly",
            "money cannot buy happiness",
            "education opens many doors",
            "teamwork achieves great results",
            "honesty is the best policy",
            "patience is a virtue",
            "practice makes perfect",
            "health is wealth",
            "knowledge is power"
        ]

    def generate_sentence(self, pattern: List[str] = None, length_limit: int = 15) -> str:
        """æ ¹æ®æ¨¡å¼ç”Ÿæˆå¥å­"""
        if pattern is None:
            pattern = random.choice(self.sentence_patterns)

        words = []
        for word_type in pattern:
            if word_type == 'article':
                word = random.choice(self.articles)
            elif word_type == 'pronoun':
                word = random.choice(self.pronouns)
            elif word_type == 'noun':
                word = random.choice(self.nouns)
            elif word_type == 'verb':
                word = random.choice(self.verbs)
            elif word_type == 'adjective':
                word = random.choice(self.adjectives)
            elif word_type == 'preposition':
                word = random.choice(self.prepositions)
            elif word_type == 'conjunction':
                word = random.choice(self.conjunctions)
            elif word_type == 'adverb':
                word = random.choice(self.adverbs)
            elif word_type == 'auxiliary':
                word = random.choice(['is', 'are', 'was', 'were', 'have', 'has', 'had', 'will', 'can', 'could'])
            elif word_type == 'question':
                word = random.choice(['?', '.', '!'])
            else:
                word = random.choice(self.nouns)

            words.append(word)

            # é¿å…å¥å­è¿‡é•¿
            if len(words) >= length_limit:
                break

        sentence = ' '.join(words)

        # ç¡®ä¿å¥å­ä»¥é€‚å½“çš„æ ‡ç‚¹ç»“å°¾
        if not sentence.endswith(('.', '!', '?')):
            sentence += random.choice(['.', '.', '!'])

        return sentence

    def generate_paragraph(self, num_sentences: int = 5, topic: str = "general") -> str:
        """ç”Ÿæˆæ®µè½"""
        sentences = []

        # å¦‚æœæœ‰ä¸»é¢˜ï¼Œå…ˆå†™ä¸€ä¸ªä¸»é¢˜å¥
        if topic != "general":
            topic_sentences = {
                "technology": ["Technology has changed our world in many ways.",
                              "Computers and smartphones are everywhere today.",
                              "The internet connects people globally."],
                "education": ["Education is very important for personal development.",
                             "Students learn many useful skills at school.",
                             "Teachers help students achieve their goals."],
                "life": ["Life is full of challenges and opportunities.",
                        "People work hard to achieve their dreams.",
                        "Family and friends provide emotional support."]
            }
            if topic in topic_sentences:
                sentences.append(random.choice(topic_sentences[topic]))

        # ç”Ÿæˆå…¶ä½™å¥å­
        for _ in range(num_sentences - len(sentences)):
            # 30%æ¦‚ç‡ä½¿ç”¨é¢„å®šä¹‰çŸ­è¯­
            if random.random() < 0.3:
                sentences.append(random.choice(self.common_phrases))
            else:
                sentences.append(self.generate_sentence())

        return ' '.join(sentences)

    def generate_dataset(self, num_samples: int = 1000,
                        min_paragraphs: int = 1,
                        max_paragraphs: int = 3,
                        topics: List[str] = None) -> List[str]:
        """ç”Ÿæˆæ•°æ®é›†"""
        if topics is None:
            topics = ["general", "technology", "education", "life"]

        dataset = []

        for i in range(num_samples):
            # éšæœºé€‰æ‹©æ®µè½æ•°
            num_paragraphs = random.randint(min_paragraphs, max_paragraphs)

            # éšæœºé€‰æ‹©ä¸»é¢˜
            topic = random.choice(topics)

            # ç”Ÿæˆæ®µè½
            paragraph = self.generate_paragraph(num_paragraphs, topic)

            dataset.append(paragraph)

            # æ¯100ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"å·²ç”Ÿæˆ {i + 1}/{num_samples} ä¸ªæ ·æœ¬")

        return dataset

    def get_story_dataset(self, num_stories: int = 100) -> List[str]:
        """ç”Ÿæˆæ•…äº‹æ•°æ®é›†"""
        stories = []

        story_templates = [
            "Once upon a time, there was a {adjective} {noun} who lived in a {adjective} {noun}. Every day, the {noun} would {verb} to the {noun}. One day, something {adjective} happened.",
            "In a {adjective} land far away, a {adjective} {noun} discovered a {adjective} {noun}. The {noun} decided to {verb} and find the {adjective} {noun}. After many {adjective} adventures, the {noun} finally {verb}.",
            "The {adjective} {noun} woke up early in the morning. Today was a {adjective} day for {verb}-ing. The {noun} went to the {noun} and met a {adjective} {noun}. Together, they {verb} to the {adjective} {noun}."
        ]

        for i in range(num_stories):
            template = random.choice(story_templates)

            # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
            story = template
            story = story.replace("{noun}", random.choice(self.nouns))
            story = story.replace("{verb}", random.choice(self.verbs))
            story = story.replace("{adjective}", random.choice(self.adjectives))

            # æ·»åŠ æ›´å¤šç»†èŠ‚
            story += f" The {random.choice(self.nouns)} was very {random.choice(self.adjectives)}."
            story += f" They {random.choice(self.verbs)} {random.choice(self.adverbs)}."
            story += " In the end, everything turned out well."

            stories.append(story)

            if (i + 1) % 20 == 0:
                print(f"å·²ç”Ÿæˆ {i + 1}/{num_stories} ä¸ªæ•…äº‹")

        return stories


class WordTokenizer:
    """è¯çº§åˆ«åˆ†è¯å™¨"""

    def __init__(self, min_freq: int = 2):
        self.min_freq = min_freq
        self.vocab = []
        self.vocab_size = 0
        self.word_to_id = {}
        self.id_to_word = {}

        # ç‰¹æ®Štoken
        self.pad_token = "<pad>"
        self.unk_token = "<unk>"
        self.start_token = "<s>"
        self.end_token = "</s>"

        self.pad_id = None
        self.unk_id = None
        self.start_id = None
        self.end_id = None

        self._add_special_tokens()

    def _add_special_tokens(self):
        """æ·»åŠ ç‰¹æ®Štoken"""
        special_tokens = [self.pad_token, self.unk_token, self.start_token, self.end_token]

        for token in special_tokens:
            if token not in self.word_to_id:
                self.word_to_id[token] = self.vocab_size
                self.id_to_word[self.vocab_size] = token
                self.vocab.append(token)
                self.vocab_size += 1

        self.pad_id = self.word_to_id[self.pad_token]
        self.unk_id = self.word_to_id[self.unk_token]
        self.start_id = self.word_to_id[self.start_token]
        self.end_id = self.word_to_id[self.end_token]

    def preprocess_text(self, text: str) -> List[str]:
        """é¢„å¤„ç†æ–‡æœ¬ï¼Œåˆ†è¯"""
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()

        # æ›¿æ¢æ ‡ç‚¹ç¬¦å·
        text = text.replace('.', ' .')
        text = text.replace(',', ' ,')
        text = text.replace('!', ' !')
        text = text.replace('?', ' ?')
        text = text.replace(';', ' ;')
        text = text.replace(':', ' :')

        # åˆ†å‰²å•è¯
        words = text.split()

        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        words = [word for word in words if word.strip()]

        return words

    def train(self, texts: List[str]):
        """è®­ç»ƒåˆ†è¯å™¨"""
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}

        for text in texts:
            words = self.preprocess_text(text)
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1

        # æ·»åŠ é«˜é¢‘è¯åˆ°è¯æ±‡è¡¨
        for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):
            if freq >= self.min_freq and word not in self.word_to_id:
                self.word_to_id[word] = self.vocab_size
                self.id_to_word[self.vocab_size] = word
                self.vocab.append(word)
                self.vocab_size += 1

        print(f"è¯çº§åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
        print(f"æ€»è¯æ•°: {len(word_freq)}, ä¿ç•™è¯æ•°: {self.vocab_size - 4}")

    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        """ç¼–ç æ–‡æœ¬"""
        words = self.preprocess_text(text)

        tokens = []
        if add_special_tokens:
            tokens.append(self.start_id)

        for word in words:
            tokens.append(self.word_to_id.get(word, self.unk_id))

        if add_special_tokens:
            tokens.append(self.end_id)

        return tokens

    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:
        """è§£ç tokenåºåˆ—"""
        words = []
        for token_id in token_ids:
            if skip_special_tokens and token_id in [self.pad_id, self.start_id, self.end_id]:
                continue
            word = self.id_to_word.get(token_id, self.unk_token)
            words.append(word)

        return ' '.join(words)

    def pad_sequences(self, sequences: List[List[int]], max_length: Optional[int] = None,
                     padding_side: str = 'right', truncation: bool = True) -> np.ndarray:
        """å¡«å……åºåˆ—"""
        if max_length is None:
            max_length = max(len(seq) for seq in sequences)

        padded_sequences = []
        for seq in sequences:
            if truncation and len(seq) > max_length:
                seq = seq[:max_length]
            elif len(seq) < max_length:
                pad_length = max_length - len(seq)
                if padding_side == 'right':
                    seq = seq + [self.pad_id] * pad_length
                else:
                    seq = [self.pad_id] * pad_length + seq
            padded_sequences.append(seq)

        return np.array(padded_sequences, dtype=np.int32)

    def save(self, filepath: str):
        """ä¿å­˜åˆ†è¯å™¨"""
        tokenizer_state = {
            'vocab': self.vocab,
            'word_to_id': self.word_to_id,
            'id_to_word': self.id_to_word,
            'vocab_size': self.vocab_size,
            'min_freq': self.min_freq,
            'pad_token': self.pad_token,
            'unk_token': self.unk_token,
            'start_token': self.start_token,
            'end_token': self.end_token
        }

        import pickle
        with open(filepath, 'wb') as f:
            pickle.dump(tokenizer_state, f)

        print(f"è¯çº§åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {filepath}")

    def load(self, filepath: str):
        """åŠ è½½åˆ†è¯å™¨"""
        import pickle
        with open(filepath, 'rb') as f:
            tokenizer_state = pickle.load(f)

        self.vocab = tokenizer_state['vocab']
        self.word_to_id = tokenizer_state['word_to_id']
        self.id_to_word = tokenizer_state['id_to_word']
        self.vocab_size = tokenizer_state['vocab_size']
        self.min_freq = tokenizer_state['min_freq']
        self.pad_token = tokenizer_state['pad_token']
        self.unk_token = tokenizer_state['unk_token']
        self.start_token = tokenizer_state['start_token']
        self.end_token = tokenizer_state['end_token']

        self.pad_id = self.word_to_id[self.pad_token]
        self.unk_id = self.word_to_id[self.unk_token]
        self.start_id = self.word_to_id[self.start_token]
        self.end_id = self.word_to_id[self.end_token]

        print(f"è¯çº§åˆ†è¯å™¨å·²ä» {filepath} åŠ è½½")


def test_improved_dataset():
    """æµ‹è¯•æ”¹è¿›çš„æ•°æ®é›†"""
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›çš„è‹±æ–‡æ•°æ®é›†...")

    # åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
    dataset_generator = EnglishTextDataset()

    # ç”Ÿæˆå°æ ·æœ¬æµ‹è¯•
    print("\\nç”Ÿæˆçš„å¥å­æ ·æœ¬:")
    for i in range(5):
        sentence = dataset_generator.generate_sentence()
        print(f"{i+1}. {sentence}")

    print("\\nç”Ÿæˆçš„æ®µè½æ ·æœ¬:")
    paragraph = dataset_generator.generate_paragraph(3, "technology")
    print(f"æŠ€æœ¯ä¸»é¢˜æ®µè½: {paragraph}")

    print("\\nç”Ÿæˆçš„æ•…äº‹æ ·æœ¬:")
    story = dataset_generator.get_story_dataset(1)[0]
    print(f"æ•…äº‹: {story}")

    # æµ‹è¯•è¯çº§åˆ†è¯å™¨
    print("\\nğŸ“š æµ‹è¯•è¯çº§åˆ†è¯å™¨...")

    # ç”Ÿæˆä¸€äº›æ–‡æœ¬
    texts = [
        "The weather is nice today.",
        "I like to read books and learn new things.",
        "She works at the university as a teacher.",
        "Technology has changed our lives completely.",
        "Education is very important for everyone."
    ]

    tokenizer = WordTokenizer(min_freq=1)
    tokenizer.train(texts)

    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"è¯æ±‡è¡¨å†…å®¹: {list(tokenizer.word_to_id.keys())[:20]}...")

    # æµ‹è¯•ç¼–ç è§£ç 
    test_text = "The weather is nice today."
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)

    print(f"åŸæ–‡: {test_text}")
    print(f"ç¼–ç : {encoded}")
    print(f"è§£ç : {decoded}")

    # æµ‹è¯•åºåˆ—å¡«å……
    sequences = [tokenizer.encode(text) for text in texts]
    padded = tokenizer.pad_sequences(sequences, max_length=15)
    print(f"\\nå¡«å……åå½¢çŠ¶: {padded.shape}")

    print("\\nâœ… æ”¹è¿›æ•°æ®é›†æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_improved_dataset()