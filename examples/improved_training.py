"""
æ”¹è¿›çš„è®­ç»ƒè„šæœ¬
ä½¿ç”¨æ›´å¥½çš„æ•°æ®å’Œæ›´é•¿çš„è®­ç»ƒæ¥æå‡è¯­è¨€èƒ½åŠ›
"""

import numpy as np
import sys
# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
import time
import os
from typing import List

from gpt_without_libs.models.core.mini_gpt import MiniGPT
from improved_dataset import EnglishTextDataset, WordTokenizer
from gpt_without_libs.training.training import Trainer, cross_entropy_loss


class ImprovedTrainer(Trainer):
    """æ”¹è¿›çš„è®­ç»ƒå™¨"""

    def __init__(self, model: MiniGPT, tokenizer: WordTokenizer):
        super().__init__(model, tokenizer)

    def prepare_batch(self, texts: List[str], batch_size: int, max_length: int) -> tuple:
        """å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡ï¼Œä½¿ç”¨è¯çº§åˆ†è¯å™¨"""
        # éšæœºé€‰æ‹©batch_sizeä¸ªæ–‡æœ¬
        selected_texts = np.random.choice(texts, batch_size, replace=True)

        # ç¼–ç æ–‡æœ¬
        sequences = []
        for text in selected_texts:
            encoded = self.tokenizer.encode(text, add_special_tokens=True)
            if len(encoded) > max_length + 1:
                encoded = encoded[:max_length + 1]
            sequences.append(encoded)

        # å¡«å……åºåˆ—
        padded = self.tokenizer.pad_sequences(sequences, max_length=max_length + 1, padding_side='right')

        # åˆ†ç¦»è¾“å…¥å’Œç›®æ ‡
        input_ids = padded[:, :-1]  # [batch_size, max_length]
        target_ids = padded[:, 1:]  # [batch_size, max_length]

        return input_ids, target_ids

    def train_step(self, input_ids: np.ndarray, target_ids: np.ndarray, learning_rate: float) -> float:
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒ"""
        # ç”Ÿæˆå› æœæ©ç 
        batch_size, seq_len = input_ids.shape
        mask = self.model.generate_mask(seq_len)

        # å‰å‘ä¼ æ’­
        logits = self.model.forward(input_ids, mask)  # [batch_size, seq_len, vocab_size]

        # è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
        loss, grad_logits = cross_entropy_loss(logits, target_ids, ignore_index=self.tokenizer.pad_id)

        # åå‘ä¼ æ’­
        self.model.backward(grad_logits)

        # æ›´æ–°å‚æ•°
        self.model.update(learning_rate)

        return loss

    def generate(self, prompt: str, max_length: int = 50, temperature: float = 1.0,
                 top_k: int = None, top_p: float = None) -> str:
        """æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆ"""
        # ç¼–ç æç¤º
        input_ids = np.array([self.tokenizer.encode(prompt, add_special_tokens=True)])
        generated_ids = input_ids[0].tolist()

        self.model.cache = {}  # æ¸…é™¤ç¼“å­˜

        for _ in range(max_length):
            # å‡†å¤‡è¾“å…¥
            current_input = np.array([generated_ids[-self.model.max_seq_len:]])

            # ç”Ÿæˆæ©ç 
            seq_len = current_input.shape[1]
            mask = self.model.generate_mask(seq_len)

            # å‰å‘ä¼ æ’­
            with np.errstate(all='ignore'):  # å¿½ç•¥æ•°å€¼è­¦å‘Š
                logits = self.model.forward(current_input, mask)

            # è·å–æœ€åä¸€ä¸ªtokençš„logits
            next_token_logits = logits[0, -1, :]

            # åº”ç”¨æ¸©åº¦
            if temperature != 1.0:
                next_token_logits = next_token_logits / temperature

            # Top-Ké‡‡æ ·
            if top_k is not None:
                top_k_indices = np.argpartition(next_token_logits, -top_k)[-top_k:]
                top_k_logits = next_token_logits[top_k_indices]
                top_k_probs = softmax(top_k_logits)
                next_token_idx = np.random.choice(top_k_indices, p=top_k_probs)
            # Top-Pé‡‡æ ·
            elif top_p is not None:
                sorted_indices = np.argsort(next_token_logits)[::-1]
                sorted_logits = next_token_logits[sorted_indices]
                sorted_probs = softmax(sorted_logits)
                cumulative_probs = np.cumsum(sorted_probs)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()
                sorted_indices_to_remove[0] = 0
                indices_to_remove = sorted_indices[sorted_indices_to_remove]
                next_token_logits[indices_to_remove] = -float('inf')
                probs = softmax(next_token_logits)
                next_token_idx = np.random.choice(len(probs), p=probs)
            else:
                # ç›´æ¥ä»æ‰€æœ‰tokenä¸­é‡‡æ ·
                probs = softmax(next_token_logits)
                next_token_idx = np.random.choice(len(probs), p=probs)

            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated_ids.append(int(next_token_idx))

            # å¦‚æœç”Ÿæˆäº†ç»“æŸtokenï¼Œåœæ­¢ç”Ÿæˆ
            if next_token_idx == self.tokenizer.end_id:
                break

        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        return generated_text


def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:
    """ç¨³å®šçš„softmaxå‡½æ•°"""
    x_max = np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)


def improved_main():
    """æ”¹è¿›çš„ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ æ”¹è¿›çš„GPTæ¨¡å‹è®­ç»ƒ - æå‡è¯­è¨€èƒ½åŠ›")
    print("=" * 80)

    # 1. ç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®
    print("\nğŸ“š ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®...")
    dataset_generator = EnglishTextDataset()

    # ç”Ÿæˆå¤šæ ·åŒ–çš„æ•°æ®
    general_texts = dataset_generator.generate_dataset(
        num_samples=300,
        min_paragraphs=2,
        max_paragraphs=4,
        topics=["general"]
    )

    tech_texts = dataset_generator.generate_dataset(
        num_samples=200,
        min_paragraphs=1,
        max_paragraphs=3,
        topics=["technology"]
    )

    edu_texts = dataset_generator.generate_dataset(
        num_samples=200,
        min_paragraphs=1,
        max_paragraphs=3,
        topics=["education"]
    )

    life_texts = dataset_generator.generate_dataset(
        num_samples=200,
        min_paragraphs=1,
        max_paragraphs=3,
        topics=["life"]
    )

    stories = dataset_generator.get_story_dataset(100)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_texts = general_texts + tech_texts + edu_texts + life_texts + stories
    print(f"æ€»å…±ç”Ÿæˆ {len(all_texts)} ä¸ªè®­ç»ƒæ ·æœ¬")

    # æ˜¾ç¤ºä¸€äº›è®­ç»ƒæ ·æœ¬
    print("\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
    for i in range(3):
        print(f"{i+1}. {all_texts[i][:100]}...")

    # 2. åˆ›å»ºè¯çº§åˆ†è¯å™¨
    print("\nğŸ”¤ åˆ›å»ºè¯çº§åˆ†è¯å™¨...")
    tokenizer = WordTokenizer(min_freq=2)
    tokenizer.train(all_texts)

    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"è¯æ±‡è¡¨ç¤ºä¾‹: {list(tokenizer.word_to_id.keys())[:15]}...")

    # 3. åˆ›å»ºæ›´å¤§çš„æ¨¡å‹
    print("\nğŸ§  åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹...")
    model = MiniGPT(
        vocab_size=tokenizer.vocab_size,
        embed_dim=128,
        num_heads=8,
        num_layers=4,
        hidden_dim=256,
        max_seq_len=128,
        dropout=0.1
    )
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")

    # 4. åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒå™¨
    print("\nğŸƒâ€â™‚ï¸ åˆå§‹åŒ–æ”¹è¿›çš„è®­ç»ƒå™¨...")
    trainer = ImprovedTrainer(model, tokenizer)

    # 5. å¼€å§‹é•¿æ—¶é—´è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹é•¿æ—¶é—´è®­ç»ƒ...")
    print("=" * 60)

    training_config = {
        'epochs': 15,  # å¢åŠ è®­ç»ƒè½®æ•°
        'batch_size': 8,  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
        'max_length': 64,  # é€‚ä¸­çš„åºåˆ—é•¿åº¦
        'learning_rate': 0.0005,  # ç¨ä½çš„å­¦ä¹ ç‡
        'save_dir': 'improved_checkpoints',
        'save_every': 20
    }

    print(f"æ”¹è¿›çš„è®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")

    start_time = time.time()

    try:
        # æ‰§è¡Œè®­ç»ƒ
        losses = trainer.train(
            texts=all_texts,
            epochs=training_config['epochs'],
            batch_size=training_config['batch_size'],
            max_length=training_config['max_length'],
            learning_rate=training_config['learning_rate'],
            save_dir=training_config['save_dir'],
            save_every=training_config['save_every']
        )

        training_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {training_time:.2f}ç§’")
        print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        print(f"æŸå¤±ä¸‹é™: {losses[0] - losses[-1]:.4f}")

        # 6. å…¨é¢æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        print("\nğŸ­ æµ‹è¯•æ”¹è¿›çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›...")
        print("=" * 60)

        test_prompts = [
            "The weather is",
            "I like to",
            "Technology has",
            "Education is",
            "In the future",
            "Once upon a time",
            "The best way to",
            "Many people believe",
            "When I was young",
            "Computers help us"
        ]

        for prompt in test_prompts:
            print(f"\nğŸ“ æç¤º: '{prompt}'")

            # ä½¿ç”¨ä¸åŒå‚æ•°ç”Ÿæˆ
            generated1 = trainer.generate(prompt, max_length=30, temperature=0.7, top_k=8)
            generated2 = trainer.generate(prompt, max_length=30, temperature=1.0, top_k=12)
            generated3 = trainer.generate(prompt, max_length=30, temperature=0.5, top_p=0.9)

            print(f"  ğŸŒ¡ï¸ T=0.7, Top-K=8:  '{generated1}'")
            print(f"  ğŸŒ¡ï¸ T=1.0, Top-K=12: '{generated2}'")
            print(f"  ğŸŒ¡ï¸ T=0.5, Top-P=0.9: '{generated3}'")
            print("-" * 50)

        # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        os.makedirs('improved_checkpoints', exist_ok=True)
        model.save("improved_checkpoints/improved_model.pkl")
        tokenizer.save("improved_checkpoints/improved_tokenizer.pkl")

        print("\n" + "=" * 80)
        print("ğŸŠ æ”¹è¿›è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
        print("âœ… æ”¹è¿›æ¨¡å‹å·²ä¿å­˜: improved_checkpoints/improved_model.pkl")
        print("âœ… æ”¹è¿›åˆ†è¯å™¨å·²ä¿å­˜: improved_checkpoints/improved_tokenizer.pkl")
        print("=" * 80)

    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    improved_main()