"""
ä½¿ç”¨çœŸå®data.txtçš„å¿«é€Ÿè®­ç»ƒç‰ˆæœ¬
é€‚åˆå¿«é€ŸéªŒè¯å’Œæµ‹è¯•
"""

import numpy as np
import sys
# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
import time
import os
from typing import List
import re

from gpt_without_libs.models.core.mini_gpt import MiniGPT
from improved_dataset import WordTokenizer
from improved_training import ImprovedTrainer


class QuickRealDataProcessor:
    """å¿«é€ŸçœŸå®æ•°æ®å¤„ç†å™¨"""

    def __init__(self, filepath: str):
        self.filepath = filepath

    def load_and_preprocess(self) -> List[str]:
        """å¿«é€ŸåŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print(f"ğŸ“‚ å¿«é€ŸåŠ è½½æ•°æ®æ–‡ä»¶: {self.filepath}")

        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            print(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ")

            # ç®€åŒ–å¤„ç†ï¼šæŒ‰æ®µè½åˆ†å‰²
            paragraphs = content.split('\n\n')
            processed_texts = []

            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                # æ¸…ç†
                paragraph = paragraph.replace('**', '').replace('*', '')
                paragraph = paragraph.replace('â€”', '-')
                paragraph = re.sub(r'\s+', ' ', paragraph)

                # è¿‡æ»¤é•¿åº¦åˆé€‚çš„æ®µè½
                if 50 <= len(paragraph) <= 300:
                    processed_texts.append(paragraph)

            print(f"âœ… å¤„ç†å®Œæˆï¼Œè·å¾— {len(processed_texts)} ä¸ªé«˜è´¨é‡æ®µè½")
            return processed_texts

        except Exception as e:
            print(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return []


def quick_real_data_main():
    """å¿«é€ŸçœŸå®æ•°æ®è®­ç»ƒä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ å¿«é€Ÿä½¿ç”¨çœŸå®data.txtæ•°æ®é›†è®­ç»ƒGPTæ¨¡å‹")
    print("=" * 80)

    # 1. åŠ è½½çœŸå®æ•°æ®
    print("\nğŸ“‚ åŠ è½½çœŸå®æ•°æ®é›†...")
    data_processor = QuickRealDataProcessor('data.txt')
    training_texts = data_processor.load_and_preprocess()

    if not training_texts:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return

    # æ•°æ®ç»Ÿè®¡
    total_chars = sum(len(text) for text in training_texts)
    total_words = sum(len(text.split()) for text in training_texts)
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ ·æœ¬æ•°: {len(training_texts)}")
    print(f"  æ€»å­—ç¬¦: {total_chars:,}")
    print(f"  æ€»å•è¯: {total_words:,}")

    # æ˜¾ç¤ºæ ·æœ¬
    print(f"\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
    for i, text in enumerate(training_texts[:3]):
        print(f"{i+1}. {text[:150]}...")

    # 2. åˆ›å»ºåˆ†è¯å™¨
    print(f"\nğŸ”¤ åˆ›å»ºè¯çº§åˆ†è¯å™¨...")
    tokenizer = WordTokenizer(min_freq=1)  # é™ä½æœ€å°é¢‘ç‡ä»¥ä¿ç•™æ›´å¤šè¯æ±‡
    tokenizer.train(training_texts)
    print(f"âœ… è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # 3. åˆ›å»ºé€‚é…çš„æ¨¡å‹
    vocab_size = tokenizer.vocab_size
    embed_dim = 128  # é€‚ä¸­çš„åµŒå…¥ç»´åº¦
    num_heads = 4    # é€‚ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¿…é¡»èƒ½æ•´é™¤embed_dimï¼‰
    num_layers = 4   # é€‚ä¸­çš„å±‚æ•°
    hidden_dim = embed_dim * 2

    print(f"\nğŸ§  åˆ›å»ºé€‚é…æ¨¡å‹:")
    print(f"  è¯æ±‡è¡¨: {vocab_size}")
    print(f"  åµŒå…¥ç»´åº¦: {embed_dim}")
    print(f"  æ³¨æ„åŠ›å¤´: {num_heads}")
    print(f"  å±‚æ•°: {num_layers}")

    model = MiniGPT(
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        hidden_dim=hidden_dim,
        max_seq_len=128,
        dropout=0.1
    )

    print(f"âœ… æ¨¡å‹å‚æ•°: {model.count_parameters():,}")

    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = ImprovedTrainer(model, tokenizer)

    # 5. å¿«é€Ÿè®­ç»ƒé…ç½®
    print(f"\nğŸ¯ å¼€å§‹å¿«é€Ÿè®­ç»ƒ...")
    print("=" * 50)

    training_config = {
        'epochs': 8,      # è¾ƒå°‘çš„è®­ç»ƒè½®æ•°
        'batch_size': 4,  # è¾ƒå°çš„æ‰¹æ¬¡å¤§å°
        'max_length': 64, # é€‚ä¸­çš„åºåˆ—é•¿åº¦
        'learning_rate': 0.001,
        'save_dir': 'quick_real_checkpoints'
    }

    print(f"å¿«é€Ÿè®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")

    start_time = time.time()

    try:
        # æ‰§è¡Œè®­ç»ƒ
        losses = trainer.train(
            texts=training_texts,
            epochs=training_config['epochs'],
            batch_size=training_config['batch_size'],
            max_length=training_config['max_length'],
            learning_rate=training_config['learning_rate'],
            save_dir=training_config['save_dir'],
            save_every=5
        )

        training_time = time.time() - start_time
        print(f"\nğŸ‰ å¿«é€Ÿè®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.2f}ç§’")
        print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")

        # 6. æµ‹è¯•çœŸå®æ•°æ®è®­ç»ƒæ•ˆæœ
        print(f"\nğŸ­ æµ‹è¯•çœŸå®æ•°æ®è®­ç»ƒæ•ˆæœ:")
        print("=" * 50)

        test_prompts = [
            "Education is",
            "Technology has",
            "Students need",
            "Learning helps",
            "In the future",
            "Programming teaches",
            "Knowledge and"
        ]

        for prompt in test_prompts:
            print(f"\nğŸ“ '{prompt}'")

            for temp in [0.6, 0.9]:
                generated = trainer.generate(prompt, max_length=30, temperature=temp, top_k=8)
                print(f"  T={temp}: '{generated}'")
            print("-" * 30)

        # 7. ä¿å­˜æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜å¿«é€Ÿè®­ç»ƒæ¨¡å‹...")
        os.makedirs('quick_real_checkpoints', exist_ok=True)
        model.save("quick_real_checkpoints/quick_real_model.pkl")
        tokenizer.save("quick_real_checkpoints/quick_real_tokenizer.pkl")

        print(f"\n" + "=" * 80)
        print(f"ğŸŠ çœŸå®æ•°æ®å¿«é€Ÿè®­ç»ƒå®Œæˆ!")
        print(f"âœ… æ¨¡å‹ä¿å­˜: quick_real_checkpoints/quick_real_model.pkl")
        print(f"âœ… åˆ†è¯å™¨: quick_real_checkpoints/quick_real_tokenizer.pkl")
        print(f"âœ… åŸºäºçœŸå®è‹±æ–‡æ–‡æœ¬ï¼Œè´¨é‡æ˜¾è‘—æå‡!")
        print("=" * 80)

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ è®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒé”™è¯¯: {e}")


if __name__ == "__main__":
    quick_real_data_main()