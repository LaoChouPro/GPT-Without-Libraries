"""
ä½¿ç”¨çœŸå®data.txtæ•°æ®é›†çš„è®­ç»ƒè„šæœ¬
ä½¿ç”¨é«˜è´¨é‡çš„è‹±æ–‡æ–‡æœ¬è¿›è¡Œè®­ç»ƒ
"""

import numpy as np
import sys
# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
import time
import os
from typing import List
import re

from gpt_without_libs.models.core.mini_gpt import MiniGPT
from improved_dataset import WordTokenizer
from improved_training import ImprovedTrainer, softmax


class RealDataProcessor:
    """çœŸå®æ•°æ®å¤„ç†å™¨"""

    def __init__(self, filepath: str):
        self.filepath = filepath

    def load_and_preprocess(self) -> List[str]:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {self.filepath}")

        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                content = f.read()

            print(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸï¼Œæ€»å­—ç¬¦æ•°: {len(content)}")

            # æ¸…ç†å’Œåˆ†å‰²æ–‡æœ¬
            processed_texts = self._process_text(content)

            print(f"âœ… å¤„ç†å®Œæˆï¼Œè·å¾— {len(processed_texts)} ä¸ªè®­ç»ƒæ ·æœ¬")
            return processed_texts

        except Exception as e:
            print(f"âŒ æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return []

    def _process_text(self, content: str) -> List[str]:
        """å¤„ç†æ–‡æœ¬å†…å®¹"""
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        content = content.replace('**', '')  # ç§»é™¤ç²—ä½“æ ‡è®°
        content = content.replace('*', '')   # ç§»é™¤æ–œä½“æ ‡è®°
        content = content.replace('â€”', '-')  # ç»Ÿä¸€ç ´æŠ˜å·
        content = content.replace('"', "'")  # ç»Ÿä¸€å¼•å·

        # æŒ‰å¥å­åˆ†å‰²ï¼ˆæ”¹è¿›çš„å¥å­åˆ†å‰²ï¼‰
        sentences = self._split_sentences(content)

        # è¿‡æ»¤å’Œæ¸…ç†å¥å­
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                # æ¸…ç†å¤šä½™ç©ºæ ¼
                sentence = re.sub(r'\s+', ' ', sentence)
                cleaned_sentences.append(sentence)

        return cleaned_sentences

    def _split_sentences(self, text: str) -> List[str]:
        """æ”¹è¿›çš„å¥å­åˆ†å‰²"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²å¥å­ï¼Œä¿ç•™æ ‡ç‚¹ç¬¦å·
        sentence_pattern = r'(?<=[.!?])\s+'
        sentences = re.split(sentence_pattern, text)

        # è¿‡æ»¤ç©ºå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]

        return sentences

    def create_training_samples(self, sentences: List[str], max_length: int = 100) -> List[str]:
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬"""
        training_samples = []

        # 1. å•å¥æ ·æœ¬
        for sentence in sentences:
            if len(sentence) <= max_length:
                training_samples.append(sentence)

        # 2. ç»„åˆå¥æ ·æœ¬ï¼ˆ2-3å¥ç»„åˆï¼‰
        for i in range(len(sentences) - 1):
            combined = sentences[i] + ' ' + sentences[i + 1]
            if len(combined) <= max_length:
                training_samples.append(combined)

        # 3. æ®µè½æ ·æœ¬ï¼ˆ4-6å¥ç»„åˆï¼‰
        for i in range(len(sentences) - 3):
            paragraph = ' '.join(sentences[i:i+4])
            if len(paragraph) <= max_length:
                training_samples.append(paragraph)

        print(f"âœ… åˆ›å»ºäº† {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return training_samples


def analyze_data_statistics(texts: List[str]):
    """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡åˆ†æ:")
    print("=" * 40)

    # åŸºæœ¬ç»Ÿè®¡
    total_chars = sum(len(text) for text in texts)
    total_words = sum(len(text.split()) for text in texts)
    avg_length = total_chars / len(texts) if texts else 0
    avg_words = total_words / len(texts) if texts else 0

    print(f"æ ·æœ¬æ•°é‡: {len(texts)}")
    print(f"æ€»å­—ç¬¦æ•°: {total_chars:,}")
    print(f"æ€»å•è¯æ•°: {total_words:,}")
    print(f"å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"å¹³å‡å•è¯æ•°: {avg_words:.1f} ä¸ª")

    # æ ‡ç‚¹ç¬¦å·ç»Ÿè®¡
    dots = sum(text.count('.') for text in texts)
    exclamation = sum(text.count('!') for text in texts)
    question = sum(text.count('?') for text in texts)
    comma = sum(text.count(',') for text in texts)
    total_punc = dots + exclamation + question + comma

    print(f"\næ ‡ç‚¹ç¬¦å·åˆ†å¸ƒ:")
    if total_punc > 0:
        print(f"å¥ç‚¹ (.): {dots} æ¬¡ ({dots/total_punc:.1%})")
        print(f"æ„Ÿå¹å· (!): {exclamation} æ¬¡ ({exclamation/total_punc:.1%})")
        print(f"é—®å· (?): {question} æ¬¡ ({question/total_punc:.1%})")
        print(f"é€—å· (,): {comma} æ¬¡ ({comma/total_punc:.1%})")

    # è¯æ±‡å¤æ‚åº¦åˆ†æ
    all_words = ' '.join(texts).lower().split()
    unique_words = len(set(all_words))
    avg_word_length = sum(len(word) for word in all_words) / len(all_words) if all_words else 0

    print(f"\nè¯æ±‡åˆ†æ:")
    print(f"æ€»è¯æ•°: {len(all_words):,}")
    print(f"ç‹¬ç‰¹è¯æ•°: {unique_words:,}")
    print(f"è¯æ±‡å¤šæ ·æ€§: {unique_words/len(all_words):.2%}")
    print(f"å¹³å‡è¯é•¿: {avg_word_length:.1f} å­—ç¬¦")


def real_data_main():
    """ä½¿ç”¨çœŸå®æ•°æ®é›†çš„ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 80)
    print("ğŸ“š ä½¿ç”¨çœŸå®data.txtæ•°æ®é›†è®­ç»ƒGPTæ¨¡å‹")
    print("=" * 80)

    # 1. åŠ è½½å’Œå¤„ç†çœŸå®æ•°æ®
    print("\nğŸ“‚ åŠ è½½çœŸå®æ•°æ®é›†...")
    data_processor = RealDataProcessor('data.txt')
    raw_sentences = data_processor.load_and_preprocess()

    if not raw_sentences:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return

    # 2. åˆ›å»ºè®­ç»ƒæ ·æœ¬
    print("\nğŸ”§ åˆ›å»ºè®­ç»ƒæ ·æœ¬...")
    training_texts = data_processor.create_training_samples(raw_sentences, max_length=150)

    # 3. æ•°æ®ç»Ÿè®¡åˆ†æ
    analyze_data_statistics(training_texts)

    # 4. æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹
    print("\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
    for i, text in enumerate(training_texts[:5]):
        print(f"{i+1}. {text[:100]}...")

    # 5. åˆ›å»ºè¯çº§åˆ†è¯å™¨
    print("\nğŸ”¤ åˆ›å»ºè¯çº§åˆ†è¯å™¨...")
    tokenizer = WordTokenizer(min_freq=2)
    tokenizer.train(training_texts)

    print(f"âœ… è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"è¯æ±‡è¡¨ç¤ºä¾‹: {list(tokenizer.word_to_id.keys())[:15]}...")

    # 6. åˆ›å»ºæ¨¡å‹ï¼ˆæ ¹æ®è¯æ±‡è¡¨å¤§å°è°ƒæ•´ï¼‰
    vocab_size = tokenizer.vocab_size
    embed_dim = min(256, max(128, vocab_size // 4))  # æ ¹æ®è¯æ±‡è¡¨å¤§å°åŠ¨æ€è°ƒæ•´
    num_heads = 8
    num_layers = 6
    hidden_dim = embed_dim * 4

    print(f"\nğŸ§  åˆ›å»ºæ¨¡å‹ (åŸºäºçœŸå®æ•°æ®ä¼˜åŒ–)...")
    print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"åµŒå…¥ç»´åº¦: {embed_dim}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print(f"Transformerå±‚æ•°: {num_layers}")
    print(f"éšè—å±‚ç»´åº¦: {hidden_dim}")

    model = MiniGPT(
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        num_heads=num_heads,
        num_layers=num_layers,
        hidden_dim=hidden_dim,
        max_seq_len=256,  # å¢åŠ åºåˆ—é•¿åº¦ä»¥å¤„ç†æ›´é•¿æ–‡æœ¬
        dropout=0.1
    )

    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")

    # 7. åˆ›å»ºè®­ç»ƒå™¨
    print("\nğŸƒâ€â™‚ï¸ åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = ImprovedTrainer(model, tokenizer)

    # 8. å¼€å§‹è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹çœŸå®æ•°æ®é›†è®­ç»ƒ...")
    print("=" * 60)

    training_config = {
        'epochs': 20,  # å¢åŠ è®­ç»ƒè½®æ•°
        'batch_size': 8,
        'max_length': 128,  # è¾ƒé•¿çš„åºåˆ—é•¿åº¦
        'learning_rate': 0.0003,  # ç¨ä½çš„å­¦ä¹ ç‡
        'save_dir': 'real_data_checkpoints',
        'save_every': 15
    }

    print(f"çœŸå®æ•°æ®è®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")

    start_time = time.time()

    try:
        # æ‰§è¡Œè®­ç»ƒ
        losses = trainer.train(
            texts=training_texts,
            epochs=training_config['epochs'],
            batch_size=training_config['batch_size'],
            max_length=training_config['max_length'],
            learning_rate=training_config['learning_rate'],
            save_dir=training_config['save_dir'],
            save_every=training_config['save_every']
        )

        training_time = time.time() - start_time
        print(f"\nğŸ‰ çœŸå®æ•°æ®è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {training_time:.2f}ç§’")
        print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
        print(f"æŸå¤±ä¸‹é™: {losses[0] - losses[-1]:.4f}")

        # 9. å…¨é¢æµ‹è¯•çœŸå®æ•°æ®è®­ç»ƒæ•ˆæœ
        print("\nğŸ­ æµ‹è¯•çœŸå®æ•°æ®è®­ç»ƒæ•ˆæœ...")
        print("=" * 60)

        test_prompts = [
            "Education is",
            "Technology has",
            "Lifelong learning",
            "In the classroom",
            "Students need",
            "Programming teaches",
            "The future of",
            "We must learn",
            "Knowledge and",
            "In conclusion"
        ]

        for prompt in test_prompts:
            print(f"\nğŸ“ æç¤º: '{prompt}'")

            # ä½¿ç”¨ä¸åŒå‚æ•°ç”Ÿæˆ
            generated1 = trainer.generate(prompt, max_length=40, temperature=0.6, top_k=8)
            generated2 = trainer.generate(prompt, max_length=40, temperature=0.9, top_k=12)
            generated3 = trainer.generate(prompt, max_length=40, temperature=1.2, top_k=15)

            print(f"  ğŸŒ¡ï¸ T=0.6: '{generated1}'")
            print(f"  ğŸŒ¡ï¸ T=0.9: '{generated2}'")
            print(f"  ğŸŒ¡ï¸ T=1.2: '{generated3}'")

            # ç»Ÿè®¡æ ‡ç‚¹ä½¿ç”¨
            dots = generated1.count('.')
            total = generated1.count('.') + generated1.count('!') + generated1.count('?') + generated1.count(',')
            if total > 0:
                dot_ratio = dots / total
                print(f"  ğŸ“Š å¥ç‚¹å æ¯”: {dot_ratio:.1%}")

            print("-" * 50)

        # 10. ä¿å­˜çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹...")
        os.makedirs('real_data_checkpoints', exist_ok=True)
        model.save("real_data_checkpoints/real_data_model.pkl")
        tokenizer.save("real_data_checkpoints/real_data_tokenizer.pkl")

        print("\n" + "=" * 80)
        print("ğŸŠ çœŸå®æ•°æ®è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
        print("âœ… çœŸå®æ•°æ®æ¨¡å‹å·²ä¿å­˜: real_data_checkpoints/real_data_model.pkl")
        print("âœ… çœŸå®æ•°æ®åˆ†è¯å™¨å·²ä¿å­˜: real_data_checkpoints/real_data_tokenizer.pkl")
        print("âœ… è®­ç»ƒåŸºäºçœŸå®çš„è‹±æ–‡æ–‡æœ¬ï¼Œè´¨é‡æ›´é«˜!")
        print("=" * 80)

    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    real_data_main()