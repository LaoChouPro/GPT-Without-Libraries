"""
ä½¿ç”¨ä¿®å¤æ•°æ®é›†çš„è®­ç»ƒè„šæœ¬
è§£å†³å¥ç‚¹è¿‡åº¦ä½¿ç”¨çš„é—®é¢˜
"""

import numpy as np
import sys
# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))
import time
import os
from typing import List

from gpt_without_libs.models.core.mini_gpt import MiniGPT
from fixed_dataset import FixedEnglishTextDataset
from improved_dataset import WordTokenizer
from improved_training import ImprovedTrainer, softmax


def fixed_main():
    """ä½¿ç”¨ä¿®å¤æ•°æ®é›†çš„ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 80)
    print("ğŸ”§ ä½¿ç”¨ä¿®å¤æ•°æ®é›†çš„GPTæ¨¡å‹è®­ç»ƒ - è§£å†³å¥ç‚¹è¿‡åº¦ä½¿ç”¨é—®é¢˜")
    print("=" * 80)

    # 1. ç”Ÿæˆä¿®å¤çš„è®­ç»ƒæ•°æ®
    print("\nğŸ“š ç”Ÿæˆä¿®å¤çš„è®­ç»ƒæ•°æ®...")
    dataset_generator = FixedEnglishTextDataset()

    # ç”Ÿæˆå¤šæ ·åŒ–çš„æ•°æ®
    simple_texts = [dataset_generator.generate_balanced_sentence() for _ in range(400)]
    paragraph_texts = [dataset_generator.generate_balanced_paragraph() for _ in range(300)]
    conversation_texts = [dataset_generator._generate_conversation() for _ in range(200)]
    mixed_texts = dataset_generator.generate_fixed_dataset(100)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_texts = simple_texts + paragraph_texts + conversation_texts + mixed_texts
    print(f"æ€»å…±ç”Ÿæˆ {len(all_texts)} ä¸ªä¿®å¤çš„è®­ç»ƒæ ·æœ¬")

    # æ˜¾ç¤ºè®­ç»ƒæ ·æœ¬å’Œæ ‡ç‚¹ç»Ÿè®¡
    print("\\nğŸ“ ä¿®å¤åçš„è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
    for i in range(5):
        print(f"{i+1}. {all_texts[i]}")

    # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å·ä½¿ç”¨
    dot_count = sum(text.count('.') for text in all_texts)
    exclamation_count = sum(text.count('!') for text in all_texts)
    question_count = sum(text.count('?') for text in all_texts)
    comma_count = sum(text.count(',') for text in all_texts)
    total_punc = dot_count + exclamation_count + question_count + comma_count

    print(f"\\nğŸ“Š æ ‡ç‚¹ç¬¦å·åˆ†å¸ƒ:")
    print(f"å¥ç‚¹ (.): {dot_count} æ¬¡ ({dot_count/total_punc:.1%})")
    print(f"æ„Ÿå¹å· (!): {exclamation_count} æ¬¡ ({exclamation_count/total_punc:.1%})")
    print(f"é—®å· (?): {question_count} æ¬¡ ({question_count/total_punc:.1%})")
    print(f"é€—å· (,): {comma_count} æ¬¡ ({comma_count/total_punc:.1%})")

    # 2. åˆ›å»ºè¯çº§åˆ†è¯å™¨
    print("\\nğŸ”¤ åˆ›å»ºè¯çº§åˆ†è¯å™¨...")
    tokenizer = WordTokenizer(min_freq=2)
    tokenizer.train(all_texts)

    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"è¯æ±‡è¡¨ç¤ºä¾‹: {list(tokenizer.word_to_id.keys())[:15]}...")

    # 3. åˆ›å»ºæ¨¡å‹
    print("\\nğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = MiniGPT(
        vocab_size=tokenizer.vocab_size,
        embed_dim=128,
        num_heads=8,
        num_layers=4,
        hidden_dim=256,
        max_seq_len=128,
        dropout=0.1
    )
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {model.count_parameters():,}")

    # 4. åˆ›å»ºè®­ç»ƒå™¨
    print("\\nğŸƒâ€â™‚ï¸ åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = ImprovedTrainer(model, tokenizer)

    # 5. å¼€å§‹è®­ç»ƒ
    print("\\nğŸ¯ å¼€å§‹ä¿®å¤æ•°æ®é›†è®­ç»ƒ...")
    print("=" * 60)

    training_config = {
        'epochs': 10,
        'batch_size': 8,
        'max_length': 64,
        'learning_rate': 0.0005,
        'save_dir': 'fixed_checkpoints',
        'save_every': 20
    }

    print(f"ä¿®å¤è®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")

    start_time = time.time()

    try:
        # æ‰§è¡Œè®­ç»ƒ
        losses = trainer.train(
            texts=all_texts,
            epochs=training_config['epochs'],
            batch_size=training_config['batch_size'],
            max_length=training_config['max_length'],
            learning_rate=training_config['learning_rate'],
            save_dir=training_config['save_dir'],
            save_every=training_config['save_every']
        )

        training_time = time.time() - start_time
        print(f"\\nğŸ‰ ä¿®å¤è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {training_time:.2f}ç§’")
        print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")

        # 6. æµ‹è¯•ä¿®å¤åçš„ç”Ÿæˆæ•ˆæœ
        print("\\nğŸ” æµ‹è¯•ä¿®å¤åçš„æ–‡æœ¬ç”Ÿæˆæ•ˆæœ...")
        print("=" * 60)

        test_prompts = [
            "The weather is",
            "I like to",
            "Technology helps",
            "Students learn",
            "What do you",
            "She works as",
            "The best way",
            "My friends"
        ]

        for prompt in test_prompts:
            print(f"\\nğŸ“ æç¤º: '{prompt}'")

            # ä½¿ç”¨ä¸åŒå‚æ•°ç”Ÿæˆ
            generated1 = trainer.generate(prompt, max_length=25, temperature=0.7, top_k=8)
            generated2 = trainer.generate(prompt, max_length=25, temperature=1.0, top_k=12)

            print(f"  ğŸŒ¡ï¸ T=0.7: '{generated1}'")
            print(f"  ğŸŒ¡ï¸ T=1.0: '{generated2}'")

            # ç»Ÿè®¡æ ‡ç‚¹ä½¿ç”¨
            dots = generated1.count('.')
            total = generated1.count('.') + generated1.count('!') + generated1.count('?') + generated1.count(',')
            if total > 0:
                dot_ratio = dots / total
                print(f"  ğŸ“Š å¥ç‚¹å æ¯”: {dot_ratio:.1%}")

            print("-" * 50)

        # 7. ä¿å­˜ä¿®å¤çš„æ¨¡å‹
        print("\\nğŸ’¾ ä¿å­˜ä¿®å¤çš„æ¨¡å‹...")
        os.makedirs('fixed_checkpoints', exist_ok=True)
        model.save("fixed_checkpoints/fixed_model.pkl")
        tokenizer.save("fixed_checkpoints/fixed_tokenizer.pkl")

        print("\\n" + "=" * 80)
        print("ğŸŠ ä¿®å¤è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
        print("âœ… ä¿®å¤æ¨¡å‹å·²ä¿å­˜: fixed_checkpoints/fixed_model.pkl")
        print("âœ… ä¿®å¤åˆ†è¯å™¨å·²ä¿å­˜: fixed_checkpoints/fixed_tokenizer.pkl")
        print("=" * 80)

    except KeyboardInterrupt:
        print("\\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    fixed_main()