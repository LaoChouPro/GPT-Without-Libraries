"""
ä¸»è®­ç»ƒè„šæœ¬
å®Œæ•´çš„æ¨¡åž‹è®­ç»ƒæ¼”ç¤º
"""

import numpy as np
import time
import os
import sys
from typing import List

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from gpt_without_libs.models.core.mini_gpt import MiniGPT
from gpt_without_libs.utils.tokenizer import SimpleTokenizer, generate_sample_data
from gpt_without_libs.training.training import Trainer


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 60)
    print("å¾®åž‹GPTæ¨¡åž‹è®­ç»ƒæ¼”ç¤º")
    print("=" * 60)

    # 1. ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("\n1. ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    num_samples = 200
    texts = generate_sample_data(num_samples, min_length=30, max_length=100)
    print(f"ç”Ÿæˆ {len(texts)} ä¸ªæ ·æœ¬")

    # 2. åˆ›å»ºåˆ†è¯å™¨
    print("\n2. è®­ç»ƒåˆ†è¯å™¨...")
    tokenizer = SimpleTokenizer()
    tokenizer.train(texts)
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # 3. åˆ›å»ºæ¨¡åž‹
    print("\n3. åˆ›å»ºæ¨¡åž‹...")
    model = MiniGPT(
        vocab_size=tokenizer.vocab_size,
        embed_dim=128,
        num_heads=4,
        num_layers=3,
        hidden_dim=256,
        max_seq_len=64,
        dropout=0.1
    )
    print(f"æ¨¡åž‹å‚æ•°æ•°é‡: {model.count_parameters():,}")

    # 4. åˆ›å»ºè®­ç»ƒå™¨
    print("\n4. åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = Trainer(model, tokenizer)

    # 5. å¼€å§‹è®­ç»ƒ
    print("\n5. å¼€å§‹è®­ç»ƒ...")
    print("=" * 40)

    training_config = {
        'epochs': 5,
        'batch_size': 4,
        'max_length': 32,
        'learning_rate': 0.001,
        'save_dir': 'checkpoints',
        'save_every': 50
    }

    print(f"è®­ç»ƒé…ç½®:")
    for key, value in training_config.items():
        print(f"  {key}: {value}")

    start_time = time.time()

    try:
        # æ‰§è¡Œè®­ç»ƒ
        losses = trainer.train(
            texts=texts,
            epochs=training_config['epochs'],
            batch_size=training_config['batch_size'],
            max_length=training_config['max_length'],
            learning_rate=training_config['learning_rate'],
            save_dir=training_config['save_dir'],
            save_every=training_config['save_every']
        )

        training_time = time.time() - start_time
        print(f"\nè®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {training_time:.2f}ç§’")

        # 6. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        print("\n6. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        print("=" * 40)

        test_prompts = [
            "The weather is",
            "I like to",
            "Computer is",
            "Good morning"
        ]

        for prompt in test_prompts:
            generated = trainer.generate(
                prompt=prompt,
                max_length=20,
                temperature=0.8,
                top_k=10
            )
            print(f"æç¤º: '{prompt}'")
            print(f"ç”Ÿæˆ: '{generated}'")
            print("-" * 30)

        # 7. ä¿å­˜æœ€ç»ˆæ¨¡åž‹å’Œåˆ†è¯å™¨
        print("\n7. ä¿å­˜æœ€ç»ˆæ¨¡åž‹...")
        model.save("mini_gpt_final.pkl")
        tokenizer.save("tokenizer_final.pkl")

        print("\n" + "=" * 60)
        print("ðŸŽ‰ è®­ç»ƒæ¼”ç¤ºå®Œæˆ!")
        print("âœ… æ¨¡åž‹å·²ä¿å­˜: mini_gpt_final.pkl")
        print("âœ… åˆ†è¯å™¨å·²ä¿å­˜: tokenizer_final.pkl")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def quick_test():
    """å¿«é€Ÿæµ‹è¯•æ¨¡å¼"""
    print("å¿«é€Ÿæµ‹è¯•æ¨¡å¼...")

    # ç”Ÿæˆå°‘é‡æ•°æ®
    texts = generate_sample_data(50, 20, 50)

    # åˆ›å»ºåˆ†è¯å™¨å’Œæ¨¡åž‹
    tokenizer = SimpleTokenizer()
    tokenizer.train(texts)

    model = MiniGPT(
        vocab_size=tokenizer.vocab_size,
        embed_dim=64,
        num_heads=2,
        num_layers=2,
        hidden_dim=128,
        max_seq_len=32
    )

    trainer = Trainer(model, tokenizer)

    # å¿«é€Ÿè®­ç»ƒå‡ æ­¥
    print("æ‰§è¡Œå¿«é€Ÿè®­ç»ƒ...")
    for step in range(5):
        input_ids, target_ids = trainer.prepare_batch(texts, batch_size=2, max_length=16)
        loss = trainer.train_step(input_ids, target_ids, learning_rate=0.001)
        print(f"æ­¥éª¤ {step + 1}, æŸå¤±: {loss:.4f}")

    # æµ‹è¯•ç”Ÿæˆ
    prompt = "Hello"
    generated = trainer.generate(prompt, max_length=15, temperature=0.7)
    print(f"æç¤º: '{prompt}' -> ç”Ÿæˆ: '{generated}'")

    print("å¿«é€Ÿæµ‹è¯•å®Œæˆ! âœ…")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        quick_test()
    else:
        main()